{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa8e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv src/.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72354d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/kushagrasharma/coding/hormozlab/src')\n",
    "\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.AutoEncoder import AE, Encoder, Decoder\n",
    "from src.Binary2LatentNN import Binary2LatentNN\n",
    "from src.Binary2TranscriptomeNN import Binary2TranscriptomeNN\n",
    "from src.utils import *\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48c75b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.environ.get(\"DATA_DIR\")\n",
    "MODELS_DIR = os.environ.get(\"MODELS_DIR\")\n",
    "\n",
    "binary_matrix_filepath = MODELS_DIR + 'binary_matrix.npy'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e2a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Data\n",
    "binary_matrix = torch.tensor(np.load(binary_matrix_filepath)).float()\n",
    "\n",
    "train_full = pd.read_csv(DATA_DIR + 'scvi_train_set_gapdh.csv', header=None).to_numpy()\n",
    "test_full = pd.read_csv(DATA_DIR + 'scvi_test_set_gapdh.csv', header=None).to_numpy()\n",
    "valid_full = pd.read_csv(DATA_DIR + 'scvi_valid_set_gapdh.csv', header=None).to_numpy()\n",
    "\n",
    "train_umap = pd.read_csv(DATA_DIR + 'train_coords.csv', header=None).to_numpy()\n",
    "test_umap = pd.read_csv(DATA_DIR + 'test_coords.csv', header=None).to_numpy()\n",
    "valid_umap = pd.read_csv(DATA_DIR + 'valid_coords.csv', header=None).to_numpy()\n",
    "\n",
    "train_tensor = torch.tensor(train_full).float()\n",
    "valid_tensor = torch.tensor(valid_full).float()\n",
    "test_tensor = torch.tensor(test_full).float()\n",
    "\n",
    "train_binary_tensor = torch.matmul(train_tensor, binary_matrix)\n",
    "valid_binary_tensor = torch.matmul(valid_tensor, binary_matrix)\n",
    "test_binary_tensor = torch.matmul(test_tensor, binary_matrix)\n",
    "\n",
    "gaussian_train = np.load(DATA_DIR + 'truncated_gaussian_sigma_10thNN.npy')\n",
    "\n",
    "closest_cell_to_valid = np.load(DATA_DIR + 'closest_cell_to_valid.npy')\n",
    "closest_cell_to_test = np.load(DATA_DIR + 'closest_cell_to_test.npy')\n",
    "\n",
    "gaussian_valid = np.apply_along_axis(lambda x: gaussian_train[x,:], 0, closest_cell_to_valid)\n",
    "gaussian_test = np.apply_along_axis(lambda x: gaussian_train[x,:], 0, closest_cell_to_test)\n",
    "\n",
    "graph = np.load(DATA_DIR + \"adjacency_15NN.npy\")\n",
    "\n",
    "N_train_cells = len(graph)\n",
    "N_valid_cells = len(valid_tensor)\n",
    "N_test_cells = len(test_tensor)\n",
    "\n",
    "### Get Laplacian\n",
    "laplacian_all = get_laplacian_from_tome_data(train_full)\n",
    "\n",
    "### Compute eigen\n",
    "lambda_all, v_all = get_laplacian_eig_from_laplacian(laplacian_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547218fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error metrics\n",
    "fn_on_matrix = lambda fn: lambda Y, Yhat: np.array([fn(Y[i,:], Yhat[i,:]) for i in range(len(Y))])\n",
    "wassersteinOnMatrix = fn_on_matrix(wasserstein_distance)\n",
    "jsOnMatrix = fn_on_matrix(jensenshannon)\n",
    "\n",
    "errorMetricOnMatrix = jsOnMatrix\n",
    "errorMetric = lambda Y, Yh: errorMetricOnMatrix(Y, Yh).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38799ae0",
   "metadata": {},
   "source": [
    "We want to understand how effective the Laplacian formulation is at allowing for a reconstruction of the Gaussian distribution. To probe into this, we first look at the mean JS Divergence when using different numbers of (true) Laplacian coefficients to reconstruct a distribution, in the validation set. \n",
    "\n",
    "Secondarily, when we perform our reconstruction using Laplacian coefficients, we need to turn our reconstruction into a probability distribution. Since we're not using all the Laplacian coefficients for many reconstructions, the resulting function is not constrained to be a distribution, so we must make entries positive and normalize. There are a few choices of ways we can make entries positive: exponentiating, squaring, and taking the absolute value. We compare the JS Divergence with respect to the number of coefficients used to make the reconstruction for all three of these methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396eec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacian_projection = np.apply_along_axis(\n",
    "    lambda x: get_laplacian_coefficients(x, v_all), 1, gaussian_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccbc5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_coefficients = list(range(1, 100, 10)) + list(range(100, 3952, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad205cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_reconstruction_n_coeffs(coeffs, n, v_all, norm_fn=lambda x: x ** 2):\n",
    "    return np.apply_along_axis(\n",
    "        lambda coeffs: laplacian_coefficients_to_probability(coeffs, v_all, norm_fn), 1, coeffs[:,:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_errors = []\n",
    "abs_errors = []\n",
    "exp_errors = []\n",
    "for n_coeff in N_coefficients:\n",
    "    laplacian_reconstruction_sq = laplacian_reconstruction_n_coeffs(laplacian_projection, n_coeff, v_all)\n",
    "    laplacian_reconstruction_abs = laplacian_reconstruction_n_coeffs(laplacian_projection, n_coeff, v_all, np.abs)\n",
    "    laplacian_reconstruction_exp = laplacian_reconstruction_n_coeffs(laplacian_projection, n_coeff, v_all, np.exp)\n",
    "    \n",
    "    sq_errors.append(errorMetricOnMatrix(laplacian_reconstruction_sq, gaussian_valid))\n",
    "    abs_errors.append(errorMetricOnMatrix(laplacian_reconstruction_abs, gaussian_valid))\n",
    "    exp_errors.append(errorMetricOnMatrix(laplacian_reconstruction_exp, gaussian_valid))\n",
    "    \n",
    "sq_errors = np.array(sq_errors)\n",
    "abs_errors = np.array(abs_errors)\n",
    "exp_errors = np.array(exp_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a010966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(N_coefficients, sq_errors.mean(axis=1), label='Error for Sq. Norm')\n",
    "plt.plot(N_coefficients, exp_errors.mean(axis=1), label='Error for Exp. Norm')\n",
    "plt.plot(N_coefficients, abs_errors.mean(axis=1), label='Error for Abs. Norm')\n",
    "plt.title(\"Number of True Laplacian Coefficients vs JS Divergence, with $\\sigma$ error bars\")\n",
    "plt.ylabel(\"JS Divergence\")\n",
    "plt.xlabel(\"Number of Laplacian Coefficients\")\n",
    "plt.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312064b",
   "metadata": {},
   "source": [
    "We see from this that the absolute norm becomes better than the squared norm for a very large number of coefficients. This makes sense, because the problem of having negative entries in the reconstructed distribution would no longer be a problem for large numbers of coefficients, because the reconstruction would be naturally a probability distribution without any post-processing. However, for the numbers of coefficients that we use in our reconstructions, the squared method is better, so we proceed with that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de6cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sq_errors.mean(axis=1), index=N_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeca484",
   "metadata": {},
   "source": [
    "We now want to see what the validation accuracy of our neural network Laplacian reconstruction method is when we vary the number of Laplacian coefficients we train to reconstruct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e0c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "toGraph = lambda X: np.apply_along_axis(lambda y: laplacian_coefficients_to_probability(y, v_all), 1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fedc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coeff_train = [1, 10, 50, 100, 250, 500, 1000, 2000, 3000, 3500, len(graph)]\n",
    "n_coeff_errors = []\n",
    "# Using a neural network to reconstruct the first 100 Laplacian eigenvalues\n",
    "# Then using the Laplacian eigenfunctions to reconstruct the distribution\n",
    "with torch.no_grad():\n",
    "    for n_coeff in n_coeff_train:\n",
    "        b2L = torch.load(MODELS_DIR + 'binaryToLaplacian{}Coeffs.pt'.format(n_coeff)).eval()\n",
    "        transf = [b2L, toGraph]\n",
    "\n",
    "        _, e = transform_and_compute_error(valid_binary_tensor, gaussian_valid, \n",
    "                                                transf, errorMetric)\n",
    "\n",
    "        n_coeff_errors.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a51c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([str(x) for x in n_coeff_train], n_coeff_errors)\n",
    "plt.xlabel(\"# of Laplacian Coeffs used\")\n",
    "plt.ylabel(\"JS Divergence vs Gaussian (Validation)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653def4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coeff_train = [1, 10, 50, 100, 250, 500, 1000, 2000, 3000, 3500, len(graph)]\n",
    "n_coeff_errors = []\n",
    "# Using a neural network to reconstruct the first 100 Laplacian eigenvalues\n",
    "# Then using the Laplacian eigenfunctions to reconstruct the distribution\n",
    "with torch.no_grad():\n",
    "    for n_coeff in n_coeff_train:\n",
    "        b2L = torch.load(MODELS_DIR + 'binaryToLaplacian{}Coeffs.pt'.format(n_coeff)).eval()\n",
    "        transf = [b2L, toGraph]\n",
    "\n",
    "        _, e = transform_and_compute_error(train_binary_tensor, gaussian_train, \n",
    "                                                transf, errorMetric)\n",
    "\n",
    "        n_coeff_errors.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([str(x) for x in n_coeff_train], n_coeff_errors)\n",
    "plt.xlabel(\"# of Laplacian Coeffs used\")\n",
    "plt.ylabel(\"JS Divergence vs Gaussian (Train)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3334c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47deffc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rna_sensor] *",
   "language": "python",
   "name": "conda-env-rna_sensor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
